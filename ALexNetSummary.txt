AlexNet contained eight layers; the first five were convolutional layers, some of them followed by max-pooling layers, and the last three were fully connected layers.[3] It used the non-saturating ReLU activation function, which showed improved training performance over tanh and sigmoid